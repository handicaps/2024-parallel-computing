# 实验报告
## LU分解
### 问题分析

LU分解的目的是将一个方阵分解成一个下三角矩阵 \( L \) 和一个上三角矩阵 \( U \)，其中 \( A = LU \)。
下面对问题进行一些分析：
#### 可以并行化的部分：
在每一列的计算过程中，可以并行计算 \( U \) 矩阵的上三角部分和 \( L \) 矩阵的下三角部分的元素。每个元素的计算都是独立的，可以并行执行。
#### 不能并行化的部分 ：
由于每一列的计算依赖于前面列的结果，所以列的计算本身需要按顺序进行。
#### 可能产生空等的地方：
在计算每一列时，由于列间的依赖关系，后面的列需要等待前面的列计算完成。这个时候部分处理器进入空闲。
#### 负载均衡划分：
可以将每一列的元素计算任务分配给不同的线程，以实现负载均衡。
#### 并行化的额外开销：
并行化会带来线程管理和同步的开销，例如在关键部分（如更新共享资源时）需要加锁，这会影响性能。以及临界区部分的操作需要处理器进行等待。

### 算法描述
使用PCAM设计方法学来描述算法设计：

#### Partitioning（划分）：将每一列中的元素计算任务划分为多个子任务，每个子任务计算一个元素。
#### Communication（通信）：线程之间需要同步计算结果，特别是在更新 L 和 U 矩阵时需要使用临界区。
#### Agglomeration（合并） ：将计算任务合理合并，减少通信开销和同步次数。
#### Mapping（映射） ：将任务映射到处理器上，使用OpenMP进行并行化，合理设置线程数。

伪代码描述如下：

1. 初始化 \( L \) 和 \( U \) 矩阵。
2. 设置最大线程数。
3. 遍历每一列 \( i \)：
   1. 计算 \( U[i][i] \)。
   2. 设置 \( L[i][i] = 1 \)。
   3. 并行计算 \( U[i][j] \) 和 \( L[j][i] \)（对于 \( j > i \)）：
      - 使用递推公式计算每个元素，并使用临界区保证线程安全。

伪代码：
```
function parallel_LU_decomposition(A, N):
    initialize matrices L and U
    set max_threads = min(omp_get_max_threads(), max(5, N / 160))
    omp_set_num_threads(max_threads)

    for i = 0 to N-1:
        U[i][i] = A[i][i] - sum_i_j_K(i, i, i)
        L[i][i] = 1

        #pragma omp parallel for
        for j = i + 1 to N-1:
            sum_u = sum_i_j_K(i, j, i)
            sum_l = sum_i_j_K(j, i, i)
            
            #pragma omp critical
            {
                U[i][j] = A[i][j] - sum_u
                L[j][i] = (A[j][i] - sum_l) / U[i][i]
            }

    return L, U
```

#### 性能优化技巧
减少临界区的使用：尽量减少临界区的使用范围，以降低同步开销。

动态负载均衡：使用OpenMP的动态调度功能，动态分配任务，平衡负载。

### 实验结果

#### 强可扩放性分析

在固定问题规模下，分别使用不同核数进行实验，并记录每次实验的执行时间。在单核情况下，时间为8.6s。
这里线程数的选择上采用了max(3, int(N / 200))的形式，是考虑到矩阵规模较大时匹配使用更多线程来处理。

| 核数 | 执行时间（秒） | 加速比 |
|------|----------------|--------|
| max(2, int(N / 200)    | 5.2             |  1.65     |
| max(3, int(N / 200))   |    4.4         |  1.95  |
| max(6, int(N / 200))   | 4.5             | 1.91   |
| max(10, int(N / 200))   | 5.4            |  1.59  |

通过实验数据可以看出，随着核数的增加，执行时间并未显著减少，甚至在核数增加到一定程度后，执行时间反而增加。这表明并行效率并不高，甚至出现了性能下降的情况。可能的原因包括：

通信开销和同步开销：线程之间的通信和同步需要时间，随着线程数增加，开销也会增加。这在并行化过程中是一个常见的问题，尤其是在任务间需要频繁同步时。

负载不均衡：任务划分可能不均衡，某些线程负载较重，导致整体性能下降。即使负载均衡策略尝试平衡任务，但实际执行中仍可能出现不均衡现象。

临界区限制：临界区的使用限制了并行度，影响了加速效果。频繁进入和退出临界区会增加开销，从而降低整体性能。

线程管理开销：创建和管理大量线程本身也会带来开销，尤其是在线程数超过一定数量时，开销会显著增加。

### 结论

通过本次实验，验证了并行计算在LU分解中的应用，可以显著提高计算效率，但其效果受多种因素影响，如通信开销、负载均衡和同步开销等。在设计并行算法时，需要综合考虑这些因素，以最大化性能提升。


## 并行化bellman-ford
## 实验报告

### 1. 算法设计

#### 1.1 问题分析

在并行化贝尔曼-福特算法时，需要考虑以下几个方面：

- 可并行化的部分：在每次迭代中，可以并行地处理每一条边，检查它们是否可以被放松。
- 空等情况：在算法的迭代过程中，如果某次迭代没有任何边被放松，则算法可以提前结束。
- 负载均衡：需要确保边被平均地分配给不同的线程，以充分利用并行计算的优势。
- 额外开销：并行化需要考虑线程管理和同步的开销，例如原子操作的使用可能导致一定的性能开销。

#### 1.2 算法描述

符合PCAM设计方法：
- Partitioning（划分）：将每次迭代中的边的放松操作划分为不同的任务，并将这些任务分配给不同的线程。
- Communication（通信）：在并行化的过程中，需要确保共享数据的一致性。在这里，使用原子操作来确保边被松弛的一致性。
- Agglomeration（合并）：每个线程独立处理一部分边的放松操作，然后将结果合并到全局的距离数组中。
- Mapping（映射）：使用OpenMP来实现并行化，通过添加OpenMP指令来告诉编译器如何并行化循环。
伪代码描述：

```
procedure parallel_bellman_ford(V, E, source, edges):
    dist = array of size V, initialized with INF except dist[source] = 0
    relaxed = True
    for i from 0 to V - 1 and relaxed:
        relaxed = False
        #pragma omp parallel for shared(relaxed)
        for j from 0 to E:
            u = edges[j].src
            v = edges[j].dest
            weight = edges[j].weight
            if dist[u] != INF and dist[u] + weight < dist[v]:
                #pragma omp atomic write
                relaxed = True
                dist[v] = dist[u] + weight
```

#### 性能优化技巧

- 原子操作优化：使用原子操作来确保在并行情况下对距离数组的更新操作的一致性，避免数据竞争和不一致性。
- 负载均衡：通过OpenMP的动态调度功能来动态分配任务，以实现负载均衡。

### 实验结果

#### 强可扩放性分析

在固定问题规模下，可以通过增加或减少线程数来测试算法在不同核数下的性能表现。通过记录不同核数下的执行时间，可以分析算法的并行效率。

| 核数 | 执行时间（秒） |
|------|---------------|
| 2    | 14.2         |
| 3    | 12.5           |

#### 性能分析

通过实验数据，可以评估算法的性能并分析未能达到线性加速的可能因素。

### 结论

在本实验中，实现了并行贝尔曼-福特算法，并对其性能进行了分析。通过合理的并行化设计和性能优化技巧，可以提高算法的执行效率，并利用多核处理器的优势来加速图的最短路径计算。


## 并行K-means
### 问题分析
#### 可并行化部分 ：
   - 数据集的划分和分配：可以将数据集分割成多个部分，每个进程负责一部分数据的处理。
   - 质心更新：各个进程并行地计算其负责数据部分的质心更新。

#### 不能并行化部分 ：
   - 初始化质心：由于质心的初始值会影响最终的聚类结果，这部分通常由主进程完成。
   - 数据的最终汇总：各个进程的计算结果需要进行汇总，以更新质心和计算总距离。

#### 可能产生空等的地方 ：
   - MPI广播和归约操作：各个进程需要同步数据和结果，这可能导致部分进程等待其他进程完成。

#### 负载均衡 ：
   - 通过将数据均匀地分配给各个进程，可以实现负载均衡。

#### 额外开销 ：
   - 并行化过程中需要进行进程间的通信，这会引入额外的时间开销。


该算法符合PCAM：

#### Partitioning（划分） ：
   - 将数据集划分为多个子集，每个子集分配给一个进程。
   - 质心的初始值由主进程计算并广播给其他进程。

#### Communication（通信） ：
   - 使用MPI的广播操作，将质心和数据大小信息广播给所有进程。
   - 各进程计算完分配的子数据集中各点到质心的最短距离后，求和，将结果发送给主进程进行汇总。
   - 主进程汇总结果后，将新的质心广播给各进程。

#### Agglomeration（聚合） ：
   - 在每次迭代中，各个进程计算其负责部分的数据点到质心的距离，更新所属质心，并计算新的质心位置。
   - 主进程汇总所有进程的计算结果，更新全局质心。

#### Mapping（映射） ：
   - 将数据均匀分配给各个进程，以实现负载均衡。

### 伪代码描述

```pseudo
Initialize MPI
if rank == 0:
    Read data
    Initialize centroids
Broadcast data and centroids to all processes
for each iteration:
    for each point in local data:
        Assign point to closest centroid
    Gather all assignments at root
    if rank == 0:
        Update centroids
    Broadcast updated centroids to all processes
Compute local total distance
Reduce total distance to root
if rank == 0:
    Output total distance
Finalize MPI
```

### 实验结果
并行加速后的执行时间为6.8-7.9s，显著加速（没有和不并行的情况比较，因为在oj平台会超时）。

#### 性能分析
未达到线性加速的可能因素：
   - 进程间通信开销：随着进程数量增加，进程间的通信开销也会增加，导致性能未能达到线性加速。
   - 负载不均衡：尽管数据均匀划分，但不同进程的数据处理时间可能不同，导致整体性能下降。

超线性加速的可能原因：
   - 缓存效应：在多核处理器上，较小的数据集可能完全适配缓存，从而提高了处理速度。
   - 进程内并行优化：某些进程在处理数据时可能进行了更有效的优化，从而导致性能超过预期。

### 结论
通过本实验，实现了MPI并行化的K-means聚类算法，并测试了其性能表现。实验结果表明，算法的并行化显著提高了执行效率，但未能达到线性加速，主要原因在于进程间通信开销和负载不均衡。



## CUDA并行化的稀疏矩阵乘法

### 问题分析
该算法采用CUDA并行化的方式实现稀疏矩阵与稠密矩阵的乘积。

#### 加载数据到GPU内存：
   - 稀疏矩阵的行偏移数组、列索引数组和值数组以及稠密矩阵数据被加载到GPU内存中。

#### 配置CUDA核函数参数：
   - 使用CUDA的线程块和网格机制，设置适当的线程块和网格大小。

#### 启动CUDA核函数：
   - 启动CUDA核函数，让GPU上的线程并行计算稀疏矩阵与稠密矩阵的乘积。
   - spmm_kernel_row_col_split 核函数采用了行-列分割的策略，即每个线程负责计算输出矩阵中的一个元素，通过对输入稠密矩阵的行和稀疏矩阵的列的并行遍历，计算输出矩阵中每个元素的值。

#### 将结果从GPU内存复制回主机内存：
   - 计算结果被从GPU全局内存复制回主机内存，并输出结果。

### 性能优化
为了提高算法的性能，考虑以下优化技巧：
- 使用更小的线程块尺寸以实现更好的负载均衡。
- 优化内存访问模式，减少全局内存的访问次数，提高存储器访问效率。

### 伪代码：
```
Input: 
    dense_matrix: 稠密矩阵
    sparse_rows, sparse_cols, sparse_vals: 稀疏矩阵数据
    M, N, P: 稠密矩阵和结果矩阵的行数
    K: 非零元素个数

Algorithm:
    1. 在GPU内存中分配空间并将稠密矩阵和稀疏矩阵数据加载到GPU全局内存中。
    2. 配置CUDA核函数参数，设置适当的线程块和网格大小。
    3. 启动CUDA核函数，在GPU上并行计算稀疏矩阵与稠密矩阵的乘积。
    4. 将计算结果从GPU全局内存复制回主机内存，并输出结果。
    5. 释放GPU内存。

CUDA Kernel Function:
    spmm_kernel_row_col_split(dense, ptr, idx, val, output, M, N, P):
        for row in range(M):
            for col in range(P):
                result = 0
                begin = ptr[col]
                end = ptr[col + 1]
                for i in range(begin, end):
                    dense_col = idx[i]
                    result += dense[row * N + dense_col] * val[i]
                output[row * P + col] = result

Main Function:
    Main():
        Read input data from stdin
        Allocate device memory for matrices and vectors
        Copy input data from host to device
        Configure CUDA kernel parameters
        Launch CUDA kernel
        Copy result from device to host
        Print result
        Free device memory

```

### 实验结果

#### 强可扩放性分析
在OJ平台，测试程序在不同块大小下的性能表现。测试结果如下：

| BLOCK_SIZE | 执行时间（秒） |
|---------|----------------|
| 32      | 4.2           |
| 16       | 3.4           |

#### 性能分析
未达到线性加速的可能因素：
   - 线程块的数量受到GPU硬件限制，随着线程块数量增加，无法线性加速。
   - 稀疏矩阵的数据结构可能导致部分线程块的计算负载过重，影响整体性能。

发现更小的线程块尺寸能够实现更好的负载均衡：
任务划分更细粒度：较小的线程块尺寸意味着每个线程块中的线程数量较少，因此每个线程块负责处理的任务量更小。这样一来，对于任务划分，可以更细粒度地将任务分配给不同的线程块，从而更好地平衡负载。
资源利用更充分：较小的线程块尺寸意味着每个线程块需要的资源更少，因此GPU的资源可以更充分地利用。较小的线程块尺寸可以减少GPU在处理较大线程块时可能出现的资源竞争和调度延迟，从而提高了GPU的利用率，有助于更好地实现负载均衡。

总的来说，通过适当调整线程块尺寸，可以更好地平衡任务的分配和资源的利用，从而实现更好的负载均衡。但线程块尺寸过小也可能会带来一些额外的开销，如线程间通信和同步开销增加。

### 结论
通过本实验，实现了CUDA并行化的稀疏矩阵乘法算法，并测试了其性能表现。实验结果表明，算法的并行化显著提高了执行效率，但未能达到线性加速，主要原因在于GPU硬件限制和计算负载不均衡。

在实验过程中，学习了CUDA并行编程的基本原理和技术，了解了GPU硬件架构对算法性能的影响。





